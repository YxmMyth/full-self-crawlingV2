"""
Prompts - Prompt æ¨¡æ¿

LLM ä»£ç ç”Ÿæˆå’Œä¿®å¤ä½¿ç”¨çš„ Prompt æ¨¡æ¿ã€‚
é€‚é… LangGraph çŠ¶æ€æœºçš„å„èŠ‚ç‚¹ã€‚

CodeAct æ¶æ„ï¼šæ‰€æœ‰å¤„ç†é€»è¾‘ç”± LLM ç”Ÿæˆ Python ä»£ç ï¼Œç„¶ååœ¨æ²™ç®±ä¸­æ‰§è¡Œã€‚
"""

import re
from typing import Optional, Dict


# ============================================================================
# å·¥å…·å‡½æ•°ï¼šæå– Python ä»£ç 
# ============================================================================

def extract_python_code(llm_response: str) -> str:
    """
    ä» LLM å“åº”ä¸­æå– Python ä»£ç 

    æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
    1. ```python ... ```
    2. ``` ... ```
    3. ç›´æ¥ä»£ç ï¼ˆæ— ä»£ç å—ï¼‰

    Args:
        llm_response: LLM è¿”å›çš„æ–‡æœ¬

    Returns:
        æå–çš„ Python ä»£ç 
    """
    # å°è¯•æå– ```python ä»£ç å—
    pattern = r'```python\n(.*?)\n```'
    match = re.search(pattern, llm_response, re.DOTALL)
    if match:
        return match.group(1).strip()

    # å°è¯•æå– ``` ä»£ç å—ï¼ˆæ— è¯­è¨€æ ‡è®°ï¼‰
    pattern = r'```\n(.*?)\n```'
    match = re.search(pattern, llm_response, re.DOTALL)
    if match:
        return match.group(1).strip()

    # æ²¡æœ‰ä»£ç å—ï¼Œç›´æ¥è¿”å›åŸæ–‡æœ¬
    return llm_response.strip()


# ============================================================================
# Sense èŠ‚ç‚¹ Prompts - DOM åˆ†æä»£ç ç”Ÿæˆ
# ============================================================================

def get_sense_dom_analysis_prompt(url: str, user_goal: str, html: str) -> str:
    """
    ç”Ÿæˆ Sense é˜¶æ®µçš„ DOM åˆ†æ Prompt
    """
    return f"""ä½ æ˜¯ä¸€ä¸ªç½‘é¡µç»“æ„åˆ†æä¸“å®¶ã€‚è¯·ç”Ÿæˆ Python ä»£ç åˆ†æä»¥ä¸‹ç½‘é¡µçš„ DOM ç»“æ„ã€‚

ã€ä»»åŠ¡ç›®æ ‡ã€‘
ç«™ç‚¹ URL: {url}
ç”¨æˆ·éœ€æ±‚: {user_goal}

ã€HTML å†…å®¹ï¼ˆå‰ 10000 å­—ç¬¦ï¼‰ã€‘
{html[:10000]}

ã€ä»£ç è¦æ±‚ã€‘
1. ä½¿ç”¨ BeautifulSoup è§£æ HTML
2. ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ã€å¯ç›´æ¥è¿è¡Œçš„ Python è„šæœ¬
3. è¾“å‡º JSON æ ¼å¼çš„åˆ†æç»“æœ

ã€è¾“å‡ºæ ¼å¼ã€‘
```json
{{
  "article_selector": "æ–‡ç« /æ¡ç›®å®¹å™¨çš„ CSS é€‰æ‹©å™¨",
  "title_selector": "æ ‡é¢˜çš„é€‰æ‹©å™¨",
  "link_selector": "é“¾æ¥çš„é€‰æ‹©å™¨",
  "pagination": {{"type": "next_page|infinite_scroll|load_more|none", "selector": "..."}},
  "sample_entries": [
    {{"title": "...", "link": "...", "extra": "..."}}
  ],
  "recommendations": ["å»ºè®®1", "å»ºè®®2"]
}}
```

ã€ä»£ç æ¨¡æ¿ã€‘
```python
from bs4 import BeautifulSoup
import json
import sys

html = '''{html[:5000]}'''

soup = BeautifulSoup(html, 'lxml')

# åˆ†æ DOM ç»“æ„
analysis = {{
    "article_selector": "è¯·æ ¹æ® HTML åˆ†æ",
    "title_selector": "è¯·æ ¹æ® HTML åˆ†æ",
    "link_selector": "è¯·æ ¹æ® HTML åˆ†æ",
    "pagination": {{"type": "none", "selector": ""}},
    "sample_entries": [],
    "recommendations": []
}}

# æå–æ ·ä¾‹æ•°æ®ï¼ˆå‰ 3 æ¡ï¼‰
# TODO: æ ¹æ® HTML ç»“æ„å®ç°

print(json.dumps(analysis, ensure_ascii=False, indent=2))
```

è¯·åªè¾“å‡º Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""


# ============================================================================
# Plan èŠ‚ç‚¹ Prompts - çˆ¬è™«ä»£ç ç”Ÿæˆ
# ============================================================================

def get_code_generation_prompt(url: str, user_goal: str, dom_analysis: str) -> str:
    """
    ç”Ÿæˆ Plan é˜¶æ®µçš„çˆ¬è™«ä»£ç ç”Ÿæˆ Prompt
    """
    # æ£€æµ‹æ˜¯å¦éœ€è¦ä»£ç ç‰‡æ®µæå–
    needs_code_extraction = _detect_code_snippet_need(user_goal)

    code_extraction_guide = ""
    if needs_code_extraction:
        code_extraction_guide = """

ã€ä»£ç ç‰‡æ®µæå–ï¼ˆSVG/HTMLï¼‰ã€‘
å¦‚æœç”¨æˆ·éœ€æ±‚åŒ…å«"SVGä»£ç "ã€"HTMLä»£ç ç‰‡æ®µ"ã€"å¯Œæ–‡æœ¬"ã€"å›¾æ ‡"ç­‰å…³é”®è¯ï¼š
- ä½¿ç”¨ `page.inner_html()` æˆ– `element.inner_html()` æå– HTML/SVG ä»£ç 
- ä½¿ç”¨ `page.evaluate("el => el.outerHTML")` è·å–åŒ…å«å…ƒç´ è‡ªèº«çš„å®Œæ•´ä»£ç 
- ç­‰å¾… JS åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆ: `page.wait_for_selector('svg', timeout=15000)`

æå–ç¤ºä¾‹ï¼š
```python
# æå– SVG ä»£ç 
svgs = page.locator("svg").all()
for svg in svgs[:5]:  # é™é‡é‡‡æ ·
    svg_code = svg.evaluate("el => el.outerHTML")
    results.append({{"svg_code": svg_code, "type": "svg"}})

# æå– HTML ç‰‡æ®µ
html_blocks = page.locator(".rich-text, .description, [data-html]").all()
for block in html_blocks[:5]:
    html_snippet = block.inner_html()
    results.append({{"html_snippet": html_snippet, "type": "html"}})
```
"""

    return f"""ä½ æ˜¯ä¸€ä¸ªçˆ¬è™«ä»£ç ç”Ÿæˆä¸“å®¶ã€‚è¯·ç”Ÿæˆå®Œæ•´çš„çˆ¬è™«ä»£ç ã€‚

ã€ä»»åŠ¡ç›®æ ‡ã€‘
ç«™ç‚¹ URL: {url}
ç”¨æˆ·éœ€æ±‚: {user_goal}

ã€DOM åˆ†æç»“æœã€‘
{dom_analysis}
{code_extraction_guide}

ã€ä»£ç è¦æ±‚ã€‘
1. ä½¿ç”¨ **playwright.sync_api**ï¼ˆåŒæ­¥æ¨¡å¼ï¼Œä¸æ˜¯ asyncï¼ï¼‰
2. æ­£ç¡®çš„ API è°ƒç”¨ï¼š
   - `browser = p.chromium.launch(headless=True)`
   - `page = browser.new_page()`  â† æ­£ç¡®ï¼
   - ä¸è¦ä½¿ç”¨ `browser.new_context()` â† é”™è¯¯ï¼
3. æå–çš„æ•°æ®ä»¥ JSON æ ¼å¼è¾“å‡ºåˆ° stdout

ã€å¸¸è§é”™è¯¯é¿å…ã€‘
| é”™è¯¯å†™æ³• | æ­£ç¡®å†™æ³• |
|---------|---------|
| `browser.new_context()` | `browser.new_page()` |
| `await page.goto()` | `page.goto()` (åŒæ­¥æ¨¡å¼) |
| `async def scrape()` | `def scrape()` (åŒæ­¥å‡½æ•°) |
| å¿˜è®° `import json` | å¿…é¡»åœ¨é¡¶éƒ¨å¯¼å…¥ |

ã€è¾“å‡ºæ ¼å¼ã€‘
```json
{{
  "results": [{{"field1": "value1", ...}}],
  "metadata": {{"total_pages": 1, "sample_size": N}}
}}
```

ã€ä»£ç æ¨¡æ¿ã€‘
```python
from playwright.sync_api import sync_playwright
import json

def scrape(url: str) -> dict:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()  # æ­£ç¡®çš„ API

        page.goto(url, wait_until='domcontentloaded', timeout=30000)

        # ç­‰å¾…å†…å®¹åŠ è½½
        try:
            page.wait_for_selector('body', timeout=10000)
        except:
            pass

        results = []

        # TODO: æ ¹æ® DOM åˆ†æç»“æœå®ç°æ•°æ®æå–
        # å‚è€ƒ: {dom_analysis[:500]}

        browser.close()

        return {{
            "results": results,
            "metadata": {{"total_pages": 1, "sample_size": len(results)}}
        }}

if __name__ == "__main__":
    import sys
    url = sys.argv[1] if len(sys.argv) > 1 else "{url}"
    result = scrape(url)
    print(json.dumps(result, ensure_ascii=False, indent=2))
```

è¯·åªè¾“å‡ºå®Œæ•´å¯æ‰§è¡Œçš„ Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""


def _detect_code_snippet_need(user_goal: str) -> bool:
    """æ£€æµ‹ç”¨æˆ·éœ€æ±‚æ˜¯å¦åŒ…å«ä»£ç ç‰‡æ®µæå–å…³é”®è¯"""
    keywords = [
        "svg", "htmlä»£ç ", "htmlç‰‡æ®µ", "html snippet",
        "ä»£ç ç‰‡æ®µ", "code snippet", "å›¾æ ‡", "icon",
        "å¯Œæ–‡æœ¬", "rich text", "ç»„ä»¶", "component",
        "å…ƒç´ ", "element", "æ ‡ç­¾", "tag"
    ]
    goal_lower = user_goal.lower()
    return any(keyword in goal_lower for keyword in keywords)


# ============================================================================
# SOOAL èŠ‚ç‚¹ Prompts - è¯Šæ–­å’Œä¿®å¤ä»£ç ç”Ÿæˆ
# ============================================================================

def get_code_diagnose_prompt(error: str, code: str) -> str:
    """
    ç”Ÿæˆ SOOAL é˜¶æ®µçš„è¯Šæ–­ Prompt
    """
    return f"""ä½ æ˜¯ä¸€ä¸ªä»£ç è¯Šæ–­ä¸“å®¶ã€‚è¯·ç”Ÿæˆ Python ä»£ç åˆ†æä»¥ä¸‹çˆ¬è™«ä»£ç æ‰§è¡Œé”™è¯¯ã€‚

ã€é”™è¯¯ä¿¡æ¯ã€‘
{error}

ã€å¤±è´¥çš„ä»£ç ã€‘
```python
{code[:3000]}
```

ã€ä»£ç è¦æ±‚ã€‘
1. ç”Ÿæˆä¸€ä¸ª Python è„šæœ¬åˆ†æé”™è¯¯ç±»å‹
2. è¾“å‡º JSON æ ¼å¼çš„è¯Šæ–­ç»“æœ

ã€è¾“å‡ºæ ¼å¼ã€‘
```json
{{
  "error_type": "selector_error|syntax_error|timeout_error|api_error|rate_limit|other",
  "root_cause": "é”™è¯¯çš„æ ¹æœ¬åŸå› æè¿°",
  "fix_suggestion": "å…·ä½“çš„ä¿®å¤å»ºè®®",
  "confidence": 0.9
}}
```

ã€é”™è¯¯ç±»å‹å‚è€ƒã€‘
- selector_error: CSS é€‰æ‹©å™¨æ‰¾ä¸åˆ°å…ƒç´ 
- syntax_error: Python è¯­æ³•é”™è¯¯
- timeout_error: é¡µé¢åŠ è½½è¶…æ—¶
- api_error: Playwright API ä½¿ç”¨é”™è¯¯
- rate_limit: è§¦å‘åçˆ¬é™åˆ¶
- other: å…¶ä»–é”™è¯¯

ã€è¯Šæ–­ä»£ç æ¨¡æ¿ã€‘
```python
import json
import re

error_text = '''{error[:1000]}'''

# åˆ†æé”™è¯¯
error_type = "other"
root_cause = "å¾…åˆ†æ"
fix_suggestion = "å¾…åˆ†æ"

# TODO: æ ¹æ® error_text åˆ¤æ–­é”™è¯¯ç±»å‹

diagnosis = {{
    "error_type": error_type,
    "root_cause": root_cause,
    "fix_suggestion": fix_suggestion,
    "confidence": 0.8
}}

print(json.dumps(diagnosis, ensure_ascii=False, indent=2))
```

è¯·åªè¾“å‡º Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""


def get_code_repair_prompt(diagnosis: str, code: str) -> str:
    """
    ç”Ÿæˆ SOOAL é˜¶æ®µçš„ä¿®å¤ Prompt
    """
    return f"""ä½ æ˜¯ä¸€ä¸ªä»£ç ä¿®å¤ä¸“å®¶ã€‚è¯·æ ¹æ®è¯Šæ–­ç»“æœä¿®å¤çˆ¬è™«ä»£ç ã€‚

ã€è¯Šæ–­ç»“æœã€‘
{diagnosis}

ã€åŸä»£ç ã€‘
```python
{code[:5000]}
```

ã€ä¿®å¤è¦æ±‚ã€‘
1. æ ¹æ®è¯Šæ–­ç»“æœä¿®å¤ä»£ç 
2. ä½¿ç”¨ **playwright.sync_api**ï¼ˆåŒæ­¥æ¨¡å¼ï¼‰
3. ç¡®ä¿ API è°ƒç”¨æ­£ç¡®ï¼š
   - `browser = p.chromium.launch(headless=True)`
   - `page = browser.new_page()` â† æ­£ç¡®ï¼
   - ä¸è¦ä½¿ç”¨ `browser.new_context()` â† é”™è¯¯ï¼
4. ç¡®ä¿è¾“å‡º JSON æ ¼å¼åŒ…å« results å’Œ metadata
5. åªè¾“å‡ºä¿®å¤åçš„å®Œæ•´ä»£ç 

è¯·ç”Ÿæˆä¿®å¤åçš„ä»£ç ã€‚
"""


# ============================================================================
# Verify èŠ‚ç‚¹ Prompts
# ============================================================================

def get_deep_validation_prompt(
    data_type: str,
    sample_items: list,
    user_goal: str,
    validation_rules: dict = None,
) -> str:
    """
    ç”Ÿæˆæ·±åº¦éªŒè¯ä»£ç çš„ Prompt

    Args:
        data_type: "image" | "pdf" | "video"
        sample_items: éœ€è¦éªŒè¯çš„æ ·æœ¬æ•°æ®ï¼ˆJSON å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
        user_goal: ç”¨æˆ·éœ€æ±‚æè¿°
        validation_rules: éªŒè¯è§„åˆ™ï¼ˆå¦‚æœ€å°åˆ†è¾¨ç‡è¦æ±‚ï¼‰

    Returns:
        å®Œæ•´çš„ Python éªŒè¯ä»£ç  Prompt
    """
    import json

    if isinstance(sample_items, list):
        sample_data_str = json.dumps(sample_items, ensure_ascii=False)
    else:
        sample_data_str = sample_items

    rules = validation_rules or {}

    if data_type == "image":
        min_resolution = rules.get("min_image_resolution", "1920x1080")
        min_width, min_height = map(int, min_resolution.split("x"))

        return f"""è¯·ç”Ÿæˆ Python ä»£ç æ·±åº¦éªŒè¯ä»¥ä¸‹å›¾ç‰‡æ•°æ®ã€‚

ã€ç”¨æˆ·éœ€æ±‚ã€‘
{user_goal}

ã€éªŒè¯è§„åˆ™ã€‘
- æœ€å°åˆ†è¾¨ç‡è¦æ±‚: {min_width}x{min_height}
- æ£€æµ‹å ä½å›¾: æ˜¯
- æ£€æµ‹ç¼©ç•¥å›¾: æ˜¯

ã€æ•°æ®æ ·æœ¬ã€‘
{sample_data_str[:2000]}

ã€ä»£ç è¦æ±‚ã€‘
ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ Python è„šæœ¬ï¼Œä½¿ç”¨ PIL (Pillow) è¿›è¡Œæ·±åº¦å›¾ç‰‡éªŒè¯ï¼š

1. **ä¸‹è½½å›¾ç‰‡**: ä½¿ç”¨ requests ä¸‹è½½å›¾ç‰‡ï¼ˆtimeout=10sï¼‰
2. **åŸºç¡€éªŒè¯**:
   - åˆ†è¾¨ç‡æ£€æŸ¥: width >= {min_width}, height >= {min_height}
   - æ ¼å¼æ£€æŸ¥: JPEG, PNG, WebP ç­‰å¸¸è§æ ¼å¼
   - å¤§å°æ£€æŸ¥: è®°å½•æ–‡ä»¶å¤§å°

3. **å ä½å›¾æ£€æµ‹**:
   - URL åŒ…å«: placeholder, default, no-image, generic, sample, example
   - ä¸­æ–‡: å ä½, é»˜è®¤
   - å°ºå¯¸è¿‡å°: width < 300 or height < 300

4. **è¾“å‡ºæ ¼å¼**:
```json
{{
  "images": [
    {{
      "url": "...",
      "valid": true,
      "width": 1920,
      "height": 1080,
      "format": "JPEG",
      "file_size_bytes": 123456,
      "is_high_res": true,
      "is_placeholder": false,
      "is_thumbnail": false
    }}
  ],
  "summary": {{
    "total": N,
    "valid": M,
    "placeholder": K,
    "low_res": L
  }}
}}
```

ã€ä»£ç æ¨¡æ¿ã€‘
```python
import requests
from PIL import Image
from io import BytesIO
import json

def validate_image(url: str) -> dict:
    '''æ·±åº¦éªŒè¯å•å¼ å›¾ç‰‡'''
    try:
        headers = {{"User-Agent": "Mozilla/5.0"}}
        response = requests.get(url, timeout=10, headers=headers)
        img = Image.open(BytesIO(response.content))

        return {{
            "url": url,
            "valid": True,
            "width": img.width,
            "height": img.height,
            "format": img.format,
            "file_size_bytes": len(response.content),
            "is_high_res": img.width >= {min_width} and img.height >= {min_height},
            "is_placeholder": any(kw in url.lower() for kw in ['placeholder', 'default', 'no-image']),
            "is_thumbnail": img.width < 300 or img.height < 300
        }}
    except Exception as e:
        return {{"url": url, "valid": False, "error": str(e)}}

# ä¸»ç¨‹åº
items = {sample_data_str[:500]}
results = []
for item in items:
    for key, value in item.items():
        if 'image' in key.lower() and isinstance(value, str):
            results.append(validate_image(value))

summary = {{
    "total": len(results),
    "valid": sum(1 for r in results if r.get("valid")),
    "placeholder": sum(1 for r in results if r.get("is_placeholder")),
    "low_res": sum(1 for r in results if not r.get("is_high_res", True))
}}

print(json.dumps({{"images": results, "summary": summary}}, ensure_ascii=False))
```

è¯·åªè¾“å‡º Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""

    elif data_type == "pdf":
        return f"""è¯·ç”Ÿæˆ Python ä»£ç æ·±åº¦éªŒè¯ä»¥ä¸‹ PDF æ•°æ®ã€‚

ã€ç”¨æˆ·éœ€æ±‚ã€‘
{user_goal}

ã€æ•°æ®æ ·æœ¬ã€‘
{sample_data_str[:2000]}

ã€ä»£ç è¦æ±‚ã€‘
ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ Python è„šæœ¬ï¼Œä½¿ç”¨ PyPDF2 è¿›è¡Œæ·±åº¦ PDF éªŒè¯ï¼š

1. **ä¸‹è½½ PDF**: ä½¿ç”¨ requests ä¸‹è½½ PDFï¼ˆtimeout=15sï¼‰
2. **åŸºç¡€éªŒè¯**:
   - é¡µæ•°æ£€æŸ¥: è®°å½•æ€»é¡µæ•°
   - å†…å®¹æ£€æŸ¥: æå–ç¬¬ä¸€é¡µæ–‡æœ¬ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å®é™…å†…å®¹
   - åŠ å¯†æ£€æŸ¥: åˆ¤æ–­æ˜¯å¦åŠ å¯†

3. **è¾“å‡ºæ ¼å¼**:
```json
{{
  "pdfs": [
    {{
      "url": "...",
      "valid": true,
      "pages": 10,
      "has_content": true,
      "is_encrypted": false,
      "file_size_bytes": 123456,
      "preview_text": "å‰200å­—ç¬¦..."
    }}
  ],
  "summary": {{
    "total": N,
    "valid": M,
    "empty_content": K
  }}
}}
```

ã€ä»£ç æ¨¡æ¿ã€‘
```python
import requests
import PyPDF2
from io import BytesIO
import json

def validate_pdf(url: str) -> dict:
    '''æ·±åº¦éªŒè¯å•ä¸ª PDF'''
    try:
        headers = {{"User-Agent": "Mozilla/5.0"}}
        response = requests.get(url, timeout=15, headers=headers)
        pdf_reader = PyPDF2.PdfReader(BytesIO(response.content))

        first_page_text = ""
        if len(pdf_reader.pages) > 0:
            first_page_text = pdf_reader.pages[0].extract_text() or ""

        return {{
            "url": url,
            "valid": True,
            "pages": len(pdf_reader.pages),
            "has_content": len(first_page_text.strip()) > 50,
            "is_encrypted": pdf_reader.is_encrypted,
            "file_size_bytes": len(response.content),
            "preview_text": first_page_text[:200]
        }}
    except Exception as e:
        return {{"url": url, "valid": False, "error": str(e)}}

# ä¸»ç¨‹åº
items = {sample_data_str[:500]}
results = []
for item in items:
    for key, value in item.items():
        if 'pdf' in key.lower() and isinstance(value, str):
            results.append(validate_pdf(value))

summary = {{
    "total": len(results),
    "valid": sum(1 for r in results if r.get("valid")),
    "empty_content": sum(1 for r in results if not r.get("has_content", True))
}}

print(json.dumps({{"pdfs": results, "summary": summary}}, ensure_ascii=False))
```

è¯·åªè¾“å‡º Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""

    elif data_type == "video":
        return f"""è¯·ç”Ÿæˆ Python ä»£ç éªŒè¯ä»¥ä¸‹è§†é¢‘æ•°æ®ã€‚

ã€ç”¨æˆ·éœ€æ±‚ã€‘
{user_goal}

ã€æ•°æ®æ ·æœ¬ã€‘
{sample_data_str[:2000]}

ã€ä»£ç è¦æ±‚ã€‘
ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ Python è„šæœ¬ï¼ŒéªŒè¯è§†é¢‘é“¾æ¥çš„å¯è®¿é—®æ€§ï¼š

1. **HEAD è¯·æ±‚**: æ£€æŸ¥é“¾æ¥æ˜¯å¦å¯è®¿é—®
2. **å†…å®¹ç±»å‹**: éªŒè¯ Content-Type æ˜¯å¦ä¸ºè§†é¢‘
3. **æ–‡ä»¶å¤§å°**: è®°å½•æ–‡ä»¶å¤§å°ï¼ˆå¦‚æœå¯ç”¨ï¼‰

ã€ä»£ç æ¨¡æ¿ã€‘
```python
import requests
import json

def validate_video(url: str) -> dict:
    '''éªŒè¯è§†é¢‘é“¾æ¥'''
    try:
        headers = {{"User-Agent": "Mozilla/5.0"}}
        response = requests.head(url, timeout=10, headers=headers, allow_redirects=True)

        content_type = response.headers.get("Content-Type", "")
        content_length = response.headers.get("Content-Length")

        is_video = "video/" in content_type

        return {{
            "url": url,
            "valid": is_video,
            "content_type": content_type,
            "file_size_bytes": int(content_length) if content_length else None,
            "accessible": response.status_code == 200
        }}
    except Exception as e:
        return {{"url": url, "valid": False, "error": str(e)}}

# ä¸»ç¨‹åº
items = {sample_data_str[:500]}
results = []
for item in items:
    for key, value in item.items():
        if 'video' in key.lower() and isinstance(value, str):
            results.append(validate_video(value))

print(json.dumps({{"videos": results}}, ensure_ascii=False))
```

è¯·åªè¾“å‡º Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""

    else:
        return get_quality_evaluation_prompt(user_goal, sample_data_str)


def get_quality_evaluation_prompt(user_goal: str, extracted_data: str) -> str:
    """
    ç”Ÿæˆ Verify é˜¶æ®µçš„è´¨é‡è¯„ä¼° Prompt

    ä¿ç•™åŸæœ‰æ¥å£ä»¥ä¿æŒå‘åå…¼å®¹ã€‚
    å†…éƒ¨è°ƒç”¨å¢å¼ºç‰ˆè¯„ä¼°å‡½æ•°ã€‚
    """
    return get_enhanced_quality_evaluation_prompt(
        user_goal=user_goal,
        extracted_data=extracted_data,
        validation_rules=None,
    )


def get_enhanced_quality_evaluation_prompt(
    user_goal: str,
    extracted_data: str,
    validation_rules: Optional[dict] = None,
) -> str:
    """
    ç”Ÿæˆå¢å¼ºçš„è´¨é‡è¯„ä¼° Prompt

    æ–°å¢éªŒè¯ç»´åº¦ï¼š
    - å›¾ç‰‡è´¨é‡: URL æœ‰æ•ˆæ€§ã€å ä½å›¾æ£€æµ‹
    - æ ¼å¼éªŒè¯: æ—¥æœŸã€ä»·æ ¼ã€URL æ ¼å¼
    - å†…å®¹è´¨é‡: éç©ºæ£€æŸ¥ã€é‡å¤æ£€æµ‹
    - ç»†ç²’åº¦éœ€æ±‚: ç”¨æˆ·è‡ªå®šä¹‰è§„åˆ™

    Args:
        user_goal: ç”¨æˆ·éœ€æ±‚æè¿°
        extracted_data: æå–çš„é‡‡æ ·æ•°æ®ï¼ˆJSON å­—ç¬¦ä¸²ï¼‰
        validation_rules: éªŒè¯è§„åˆ™å­—å…¸ï¼ˆå¯é€‰ï¼‰

    Returns:
        å®Œæ•´çš„è´¨é‡è¯„ä¼° Prompt
    """
    rules = validation_rules or {}

    return f"""è¯·ç”Ÿæˆ Python ä»£ç è¯„ä¼°ä»¥ä¸‹é‡‡æ ·æ•°æ®çš„è´¨é‡ã€‚

ã€ç”¨æˆ·éœ€æ±‚ã€‘
{user_goal}

ã€æå–çš„æ•°æ®ã€‘
{extracted_data}

ã€éªŒè¯è§„åˆ™ã€‘
{{"check_duplicates": {rules.get("check_duplicates", True)},
 "validate_urls": {rules.get("validate_urls", True)},
 "validate_images": {rules.get("validate_images", False)},
 "validate_price": {rules.get("validate_price", False)},
 "validate_date": {rules.get("validate_date", False)}}}

ã€ä»£ç è¦æ±‚ã€‘
ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ Python è„šæœ¬ï¼ŒåŒ…å«ä»¥ä¸‹éªŒè¯å‡½æ•°ï¼š

1. **validate_images(items)**: å›¾ç‰‡è´¨é‡éªŒè¯
   - æ£€æŸ¥å›¾ç‰‡ URL æ ¼å¼æœ‰æ•ˆæ€§
   - æ£€æµ‹å ä½å›¾ï¼ˆåŒ…å« "placeholder", "default", "no-image" ç­‰ï¼‰
   - è¿”å›: {{"total": N, "valid": M, "placeholder": K, "invalid": L}}

2. **validate_formats(items)**: æ ¼å¼éªŒè¯
   - æ—¥æœŸæ ¼å¼: YYYY-MM-DD, ISO 8601 ç­‰
   - ä»·æ ¼æ ¼å¼: æ•°å­— + è´§å¸ç¬¦å·
   - URL æ ¼å¼: æœ‰æ•ˆçš„ http(s) URL
   - è¿”å›: {{"date_valid": N, "price_valid": M, "url_valid": K}}

3. **validate_content(items)**: å†…å®¹è´¨é‡éªŒè¯
   - æ£€æŸ¥å¿…å¡«å­—æ®µæ˜¯å¦ä¸ºç©º
   - æ£€æµ‹é‡å¤è®°å½•ï¼ˆåŸºäºæ ‡é¢˜/é“¾æ¥å»é‡ï¼‰
   - æ£€æµ‹æ— æ„ä¹‰å†…å®¹ï¼ˆ"N/A", "null", "å¾…è¡¥å……"ï¼‰
   - è¿”å›: {{"empty_fields": N, "duplicates": M, "invalid_content": K}}

4. **calculate_quality_score(items)**: ç»¼åˆè¯„åˆ†
   - relevance (0.4): ä¸ç”¨æˆ·éœ€æ±‚çš„ç›¸å…³æ€§ï¼ˆæ ¹æ®å­—æ®µåŒ¹é…åº¦åˆ¤æ–­ï¼‰
   - completeness (0.3): å¿…å¡«å­—æ®µå®Œæ•´åº¦
   - accuracy (0.2): æ ¼å¼æ­£ç¡®æ€§
   - content_quality (0.1): å†…å®¹è´¨é‡ï¼ˆéç©ºã€æ— é‡å¤ï¼‰

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·è¾“å‡º JSON æ ¼å¼çš„è¯„ä¼°ç»“æœï¼š

```json
{{
  "relevance": 0.9,
  "completeness": 0.8,
  "accuracy": 0.95,
  "content_quality": 0.7,
  "overall_score": 0.85,
  "image_stats": {{"total": 50, "valid": 45, "placeholder": 5, "invalid": 0}},
  "format_stats": {{"date_valid": 48, "date_total": 50, "price_valid": 50, "price_total": 50, "url_valid": 49, "url_total": 50}},
  "content_stats": {{"empty_fields": 2, "duplicates": 0, "invalid_content": 1, "total_items": 50}},
  "issues": ["å…·ä½“é—®é¢˜æè¿°..."],
  "suggestions": ["æ”¹è¿›å»ºè®®..."]
}}
```

ã€æ•°æ®å®šä¹‰ã€‘
è¯·ä½¿ç”¨ä»¥ä¸‹æ•°æ®å®šä¹‰ï¼š

```python
import json
from urllib.parse import urlparse
from datetime import datetime
import re

# è¾“å…¥æ•°æ®
items = {extracted_data}

def validate_images(items: list) -> dict:
    '''éªŒè¯å›¾ç‰‡è´¨é‡'''
    stats = {{"total": 0, "valid": 0, "placeholder": 0, "invalid": 0}}
    placeholder_keywords = ['placeholder', 'default', 'no-image', 'no_image',
                           'generic', 'sample', 'example', 'empty', 'missing',
                           'å ä½', 'é»˜è®¤']

    for item in items:
        for key, value in item.items():
            if 'image' in key.lower() or 'img' in key.lower() or 'picture' in key.lower() or 'photo' in key.lower():
                if isinstance(value, str) and value:
                    stats["total"] += 1
                    # æ£€æŸ¥ URL æœ‰æ•ˆæ€§
                    try:
                        result = urlparse(value)
                        if not all([result.scheme in ['http', 'https'], result.netloc]):
                            stats["invalid"] += 1
                            continue
                    except:
                        stats["invalid"] += 1
                        continue

                    # æ£€æŸ¥å ä½å›¾
                    if any(kw in value.lower() for kw in placeholder_keywords):
                        stats["placeholder"] += 1
                    else:
                        stats["valid"] += 1

    return stats

def validate_formats(items: list) -> dict:
    '''éªŒè¯æ•°æ®æ ¼å¼'''
    stats = {{"date_valid": 0, "date_total": 0,
              "price_valid": 0, "price_total": 0,
              "url_valid": 0, "url_total": 0}}

    # æ—¥æœŸæ ¼å¼æ¨¡å¼
    date_patterns = [
        r'^\\d{{4}}-\\d{{2}}-\\d{{2}}$',           # YYYY-MM-DD
        r'^\\d{{4}}/\\d{{2}}/\\d{{2}}$',           # YYYY/MM/DD
        r'^\\d{{4}}å¹´\\d{{1,2}}æœˆ\\d{{1,2}}æ—¥$',  # ä¸­æ–‡æ—¥æœŸ
    ]

    # ä»·æ ¼æ ¼å¼æ¨¡å¼
    price_pattern = r'^[Â¥$â‚¬Â£]?\\s*\\d+(\\.\\d+)?\\s*[å…ƒç¾å…ƒEURGBPUSD]?$'

    for item in items:
        for key, value in item.items():
            if not isinstance(value, str):
                continue

            # æ—¥æœŸéªŒè¯
            if 'date' in key.lower() or 'time' in key.lower() or 'æ—¶é—´' in key or 'æ—¥æœŸ' in key:
                stats["date_total"] += 1
                if any(re.match(p, value.strip()) for p in date_patterns):
                    stats["date_valid"] += 1

            # ä»·æ ¼éªŒè¯
            elif 'price' in key.lower() or 'æˆæœ¬' in key or 'ä»·æ ¼' in key or 'è´¹ç”¨' in key:
                stats["price_total"] += 1
                if re.match(price_pattern, value.strip()):
                    stats["price_valid"] += 1

            # URL éªŒè¯
            elif 'url' in key.lower() or 'link' in key.lower() or 'href' in key.lower() or 'é“¾æ¥' in key:
                stats["url_total"] += 1
                try:
                    result = urlparse(value)
                    if all([result.scheme in ['http', 'https'], result.netloc]):
                        stats["url_valid"] += 1
                except:
                    pass

    return stats

def validate_content(items: list) -> dict:
    '''éªŒè¯å†…å®¹è´¨é‡'''
    stats = {{
        "empty_fields": 0,
        "duplicates": 0,
        "invalid_content": 0,
        "total_items": len(items)
    }}

    seen = set()
    null_values = ["n/a", "null", "none", "å¾…è¡¥å……", "æš‚æ— ", "tbd", "-", "â€”",
                   "undefined", "unknown", "?"]

    for item in items:
        # æ£€æŸ¥é‡å¤ï¼ˆåŸºäºæ ‡é¢˜æˆ–é“¾æ¥ï¼‰
        identifier = item.get("title") or item.get("url") or item.get("link") or str(item.get("id", ""))
        if identifier and identifier in seen:
            stats["duplicates"] += 1
        seen.add(identifier)

        # æ£€æŸ¥ç©ºå­—æ®µå’Œæ— æ„ä¹‰å†…å®¹
        for value in item.values():
            if value is None or value == "":
                stats["empty_fields"] += 1
            elif isinstance(value, str):
                val_stripped = value.strip()
                if not val_stripped:
                    stats["empty_fields"] += 1
                elif val_stripped.lower() in null_values:
                    stats["invalid_content"] += 1

    return stats

def calculate_quality_score(items: list, image_stats: dict, format_stats: dict, content_stats: dict) -> dict:
    '''è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°'''
    total_items = len(items)
    if total_items == 0:
        return {{"relevance": 0, "completeness": 0, "accuracy": 0, "content_quality": 0, "overall_score": 0}}

    # relevance: åŸºäºæ•°æ®ä¸°å¯Œåº¦ï¼ˆå¹³å‡æ¯æ¡è®°å½•çš„å­—æ®µæ•°ï¼‰
    avg_fields = sum(len([v for v in item.values() if v not in [None, ""]]) for item in items) / total_items
    relevance = min(1.0, avg_fields / 5)  # å‡è®¾ 5 ä¸ªå­—æ®µä¸ºæ»¡åˆ†

    # completeness: åŸºäºéç©ºå­—æ®µæ¯”ä¾‹
    total_fields = sum(len(item) for item in items)
    filled_fields = total_fields - content_stats.get("empty_fields", 0)
    completeness = filled_fields / total_fields if total_fields > 0 else 0

    # accuracy: åŸºäºæ ¼å¼éªŒè¯é€šè¿‡ç‡
    format_valid = 0
    format_total = 0
    for k in ["date_total", "price_total", "url_total"]:
        if format_stats.get(k, 0) > 0:
            format_total += format_stats[k]
            valid_key = k.replace("_total", "_valid")
            format_valid += format_stats.get(valid_key, 0)
    accuracy = format_valid / format_total if format_total > 0 else 0.8

    # content_quality: åŸºäºå†…å®¹è´¨é‡ï¼ˆæ— é‡å¤ã€æ— æ— æ•ˆå†…å®¹ï¼‰
    content_quality = 1.0
    if content_stats.get("total_items", 0) > 0:
        dup_ratio = content_stats.get("duplicates", 0) / content_stats["total_items"]
        invalid_ratio = content_stats.get("invalid_content", 0) / max(content_stats["total_items"] * 3, 1)
        content_quality = max(0, 1.0 - dup_ratio - invalid_ratio)

    # ç»¼åˆå¾—åˆ†
    overall_score = (relevance * 0.4 + completeness * 0.3 + accuracy * 0.2 + content_quality * 0.1)

    return {{
        "relevance": round(relevance, 2),
        "completeness": round(completeness, 2),
        "accuracy": round(accuracy, 2),
        "content_quality": round(content_quality, 2),
        "overall_score": round(overall_score, 2)
    }}

# ä¸»ç¨‹åº
if __name__ == "__main__":
    image_stats = validate_images(items)
    format_stats = validate_formats(items)
    content_stats = validate_content(items)
    scores = calculate_quality_score(items, image_stats, format_stats, content_stats)

    # æ”¶é›†é—®é¢˜
    issues = []
    if scores["completeness"] < 0.7:
        issues.append(f"æ•°æ®å®Œæ•´æ€§è¾ƒä½: {{scores['completeness']}}ï¼Œéƒ¨åˆ†å¿…å¡«å­—æ®µå¯èƒ½ç¼ºå¤±")
    if image_stats.get("placeholder", 0) > 0:
        issues.append(f"å‘ç°å ä½å›¾: {{image_stats['placeholder']}} ä¸ª")
    if content_stats.get("duplicates", 0) > 0:
        issues.append(f"å‘ç°é‡å¤è®°å½•: {{content_stats['duplicates']}} æ¡")

    result = {{
        **scores,
        "image_stats": image_stats,
        "format_stats": format_stats,
        "content_stats": content_stats,
        "issues": issues,
        "suggestions": []
    }}

    print(json.dumps(result, ensure_ascii=False, indent=2))
```

è¯·åªè¾“å‡º Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""


def extract_validation_rules(user_goal: str) -> dict:
    """
    ä»ç”¨æˆ·éœ€æ±‚ä¸­æå–éªŒè¯è§„åˆ™

    Args:
        user_goal: ç”¨æˆ·éœ€æ±‚æè¿°

    Returns:
        éªŒè¯è§„åˆ™å­—å…¸

    ç¤ºä¾‹:
        - "æå–é«˜æ¸…å›¾ç‰‡" â†’ {{"image_quality": "high"}}
        - "ä»·æ ¼æ ¼å¼è¦æ­£ç¡®" â†’ {{"validate_price": True}}
        - "ä¸èƒ½æœ‰é‡å¤" â†’ {{"check_duplicates": True}}
    """
    rules = {
        "check_duplicates": True,
        "validate_urls": True,
    }

    goal_lower = user_goal.lower()

    # å›¾ç‰‡ç›¸å…³
    if "å›¾ç‰‡" in goal_lower or "image" in goal_lower or "img" in goal_lower:
        rules["validate_images"] = True
        if "é«˜æ¸…" in goal_lower or "high" in goal_lower or "hd" in goal_lower:
            rules["image_quality"] = "high"

    # ä»·æ ¼ç›¸å…³
    if "ä»·æ ¼" in goal_lower or "price" in goal_lower or "æˆæœ¬" in goal_lower or "è´¹ç”¨" in goal_lower:
        rules["validate_price"] = True

    # æ—¥æœŸç›¸å…³
    if "æ—¥æœŸ" in goal_lower or "date" in goal_lower or "æ—¶é—´" in goal_lower or "time" in goal_lower:
        rules["validate_date"] = True

    # å»é‡ç›¸å…³
    if "ä¸é‡å¤" in goal_lower or "unique" in goal_lower or "å»é‡" in goal_lower:
        rules["check_duplicates"] = True

    # é“¾æ¥ç›¸å…³
    if "é“¾æ¥" in goal_lower or "url" in goal_lower or "link" in goal_lower:
        rules["validate_urls"] = True

    return rules


# ============================================================================
# Report èŠ‚ç‚¹ Prompts
# ============================================================================

def get_report_generation_prompt(
    site_url: str,
    user_goal: str,
    site_info: str,
    sample_data: str,
    sool_iteration: int,
    quality_score: float,
    sample_count: int,
) -> str:
    """
    ç”Ÿæˆ Report é˜¶æ®µçš„æŠ¥å‘Šç”Ÿæˆ Prompt
    """
    return f"""è¯·ç”Ÿæˆç½‘ç«™ä¾¦å¯ŸæŠ¥å‘Šçš„ Markdown æ ¼å¼ã€‚

ã€ç«™ç‚¹ä¿¡æ¯ã€‘
- URL: {site_url}
- ç”¨æˆ·éœ€æ±‚: {user_goal}

ã€ç«™ç‚¹ä¸Šä¸‹æ–‡ã€‘
{site_info[:1000]}

ã€æ ·æœ¬æ•°æ®ã€‘ï¼ˆå‰ 5 æ¡ï¼‰
{sample_data[:1000]}

ã€ä¾¦å¯Ÿç»Ÿè®¡ã€‘
- SOOAL è¿­ä»£æ¬¡æ•°: {sool_iteration}
- è´¨é‡åˆ†æ•°: {quality_score}
- æ ·æœ¬æ•°é‡: {sample_count}

ã€æŠ¥å‘Šè¦æ±‚ã€‘
ç”Ÿæˆç»“æ„åŒ–çš„ä¾¦å¯ŸæŠ¥å‘Šï¼ŒåŒ…å«ï¼š
1. ç«™ç‚¹åŸºæœ¬ä¿¡æ¯
2. æ•°æ®ä¾¦å¯Ÿç»“æœï¼ˆä¼°ç®—æ€»é‡ã€æ ·æœ¬è´¨é‡ï¼‰
3. çœŸå®æ ·æœ¬é¢„è§ˆ
4. å¯çˆ¬æ€§è¯„ä¼°
5. æ¨èçˆ¬å–ç­–ç•¥

ã€è¾“å‡ºæ ¼å¼ç¤ºä¾‹ã€‘
```markdown
# ç½‘ç«™æ•°æ®ä¾¦å¯ŸæŠ¥å‘Š

## ç«™ç‚¹ä¿¡æ¯
- URL: {site_url}
- ç”¨æˆ·éœ€æ±‚: {user_goal}
- ä¾¦å¯Ÿæ—¶é—´: 2026-XX-XX

## ä¾¦å¯Ÿæ€»ç»“
- ä¼°ç®—æ•°æ®æ€»é‡: ~1000 æ¡
- æ ·æœ¬è´¨é‡åˆ†æ•°: {quality_score}/1.0
- æ•°æ®ç»“æ„åŒ–ç¨‹åº¦: é«˜

## ç«™ç‚¹ç‰¹å¾åˆ†æ
- é¡µé¢ç±»å‹: åˆ—è¡¨é¡µ
- åˆ†é¡µæ–¹å¼: ä¼ ç»Ÿåˆ†é¡µ
- åçˆ¬ç­‰çº§: ä½

## çœŸå®æ ·æœ¬é¢„è§ˆ
{{æ ·æœ¬æ•°æ®}}

## å¯çˆ¬æ€§è¯„ä¼°
- åçˆ¬ç­‰çº§: ä½/ä¸­/é«˜
- æŠ€æœ¯éš¾åº¦: ç®€å•/ä¸­ç­‰/å¤æ‚
- æ¨èç­–ç•¥: [å…·ä½“ç­–ç•¥å»ºè®®]
```

è¯·ç”ŸæˆæŠ¥å‘Šã€‚
"""


# ============================================================================
# Interact èŠ‚ç‚¹ Prompts - å¤šæ­¥äº¤äº’é€»è¾‘
# ============================================================================

def get_interact_prompt(
    url: str,
    user_goal: str,
    dom_analysis: str = "",
    detected_features: list = None,
) -> str:
    """
    ç”Ÿæˆäº¤äº’é˜¶æ®µçš„ Prompt

    ç”¨äºå¤„ç†éœ€è¦å¤šæ­¥äº¤äº’çš„åœºæ™¯ï¼Œä¾‹å¦‚ï¼š
    1. ç‚¹å‡»æœç´¢æŒ‰é’®
    2. å¡«å†™è¡¨å•
    3. æ»šåŠ¨åŠ è½½
    4. ç­‰å¾…åŠ¨æ€å†…å®¹

    Args:
        url: ç›®æ ‡ç«™ç‚¹ URL
        user_goal: ç”¨æˆ·éœ€æ±‚
        dom_analysis: DOM åˆ†æç»“æœ
        detected_features: æ£€æµ‹åˆ°çš„é¡µé¢ç‰¹å¾

    Returns:
        äº¤äº’ä»£ç ç”Ÿæˆ Prompt
    """
    features = detected_features or []

    features_text = ""
    if features:
        features_text = f"\nã€æ£€æµ‹åˆ°çš„é¡µé¢ç‰¹å¾ã€‘\n{', '.join(features)}"

    return f"""ä½ æ˜¯ä¸€ä¸ªæµè§ˆå™¨äº¤äº’ä¸“å®¶ã€‚è¯·ç”Ÿæˆå¤„ç†å¤šæ­¥äº¤äº’çš„ä»£ç ã€‚

ã€ä»»åŠ¡ç›®æ ‡ã€‘
ç«™ç‚¹ URL: {url}
ç”¨æˆ·éœ€æ±‚: {user_goal}
{features_text}

ã€DOM åˆ†æç»“æœã€‘
{dom_analysis[:1000] if dom_analysis else "æš‚æ— "}

ã€å¸¸è§äº¤äº’ç±»å‹ã€‘

1. **ç‚¹å‡»æŒ‰é’®è§¦å‘å†…å®¹åŠ è½½**
   ```python
   # æŸ¥æ‰¾å¹¶ç‚¹å‡»æœç´¢/æäº¤æŒ‰é’®
   search_btn = page.query_selector('button[type="submit"]')
   if not search_btn:
       search_btn = page.query_selector('button:has-text("Search")')
   if search_btn:
       search_btn.click()
       page.wait_for_timeout(2000)  # ç­‰å¾…å†…å®¹åŠ è½½
   ```

2. **å¡«å†™è¡¨å•å¹¶æäº¤**
   ```python
   # å¡«å†™æœç´¢æ¡†
   search_input = page.query_selector('input[name="search"], input[placeholder*="search" i]')
   if search_input:
       search_input.fill('keywords')
       search_input.press('Enter')
       page.wait_for_selector('.results, .items', timeout=5000)
   ```

3. **æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹**
   ```python
   # å¤šæ¬¡æ»šåŠ¨ä»¥åŠ è½½æ‰€æœ‰å†…å®¹
   for _ in range(3):
       page.evaluate('window.scrollBy(0, window.innerHeight)')
       page.wait_for_timeout(1000)
   ```

4. **æ·»åŠ  URL å‚æ•°ï¼ˆé€‚ç”¨äºæœç´¢é¡µï¼‰**
   ```python
   # å¦‚æœé¡µé¢æ˜¯æœç´¢é¡µä½†æ²¡æœ‰ç»“æœï¼Œå°è¯•æ·»åŠ å‚æ•°
   current_url = page.url
   if '?' not in current_url:
       page.goto(current_url + '?search=&page=1')
       page.wait_for_timeout(2000)
   ```

5. **ç‚¹å‡»"å±•å¼€æ›´å¤š"é“¾æ¥**
   ```python
   # æŸ¥æ‰¾å¹¶ç‚¹å‡»å±•å¼€é“¾æ¥
   expand_links = page.query_selector_all('a:has-text("more"), a:has-text("å±•å¼€"), button:has-text("show")')
   for link in expand_links[:3]:
       try:
           link.click()
           page.wait_for_timeout(500)
       except:
           pass
   ```

ã€ä»£ç è¦æ±‚ã€‘
1. ä½¿ç”¨ **playwright.sync_api**ï¼ˆåŒæ­¥æ¨¡å¼ï¼‰
2. å®šä¹‰ `interact(page)` å‡½æ•°ï¼Œæ‰§è¡Œäº¤äº’åè¿”å›æœ€ç»ˆ URL
3. æ·»åŠ é€‚å½“çš„ç­‰å¾…å’Œé”™è¯¯å¤„ç†
4. è¾“å‡º JSON æ ¼å¼åŒ…å« final_url å’Œ interactions è®°å½•

ã€è¾“å‡ºæ ¼å¼ã€‘
```json
{{
  "final_url": "äº¤äº’åçš„é¡µé¢ URL",
  "interactions": ["ç‚¹å‡»äº†æœç´¢æŒ‰é’®", "ç­‰å¾…äº†2ç§’"],
  "success": true
}}
```

ã€ä»£ç æ¨¡æ¿ã€‘
```python
from playwright.sync_api import sync_playwright
import json

def interact(page) -> str:
    '''æ‰§è¡Œå¤šæ­¥äº¤äº’ï¼Œè¿”å›æœ€ç»ˆ URL'''

    interactions = []

    # ç¤ºä¾‹ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦ç‚¹å‡»æœç´¢æŒ‰é’®
    try:
        # æŸ¥æ‰¾å¯èƒ½çš„æœç´¢æŒ‰é’®
        search_selectors = [
            'button[type="submit"]',
            'button:has-text("Search")',
            'input[type="submit"]',
            'button:has-text("æœç´¢")',
        ]

        search_btn = None
        for selector in search_selectors:
            search_btn = page.query_selector(selector)
            if search_btn:
                interactions.append(f"æ‰¾åˆ°æœç´¢æŒ‰é’®: {{selector}}")
                break

        if search_btn:
            search_btn.click()
            interactions.append("ç‚¹å‡»äº†æœç´¢æŒ‰é’®")
            page.wait_for_timeout(2000)  # ç­‰å¾…ç»“æœåŠ è½½
    except Exception as e:
        interactions.append(f"ç‚¹å‡»æœç´¢æŒ‰é’®å¤±è´¥: {{str(e)}}")

    # ç¤ºä¾‹ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ  URL å‚æ•°
    current_url = page.url
    if '?' not in current_url and 'search' in current_url.lower():
        page.goto(current_url + '?keywords=')
        interactions.append("æ·»åŠ äº† URL å‚æ•°")
        page.wait_for_timeout(2000)

    return page.url, interactions

def main():
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        page.goto("{url}", wait_until='domcontentloaded', timeout=30000)

        # ç­‰å¾…é¡µé¢åˆå§‹åŠ è½½
        try:
            page.wait_for_selector('body', timeout=5000)
        except:
            pass

        # æ‰§è¡Œäº¤äº’
        final_url, interactions = interact(page)

        result = {{
            "final_url": final_url,
            "interactions": interactions,
            "success": final_url != page.url  # URL å˜åŒ–è¯´æ˜å¯èƒ½å‘ç”Ÿäº†äº¤äº’
        }}

        print(json.dumps(result, ensure_ascii=False))

        browser.close()

if __name__ == "__main__":
    main()
```

è¯·åªè¾“å‡ºå®Œæ•´å¯æ‰§è¡Œçš„ Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""


def get_enhanced_sense_prompt(
    url: str,
    user_goal: str,
    html: str,
    user_goal_requires_interaction: bool = False,
) -> str:
    """
    ç”Ÿæˆå¢å¼ºçš„ Sense é˜¶æ®µ Prompt

    æ–°å¢åŠŸèƒ½ï¼š
    1. é€‰æ‹©å™¨éªŒè¯
    2. æ£€æµ‹æ˜¯å¦éœ€è¦äº¤äº’
    3. åˆ†æ DOM ç»“æ„

    Args:
        url: ç›®æ ‡ç«™ç‚¹ URL
        user_goal: ç”¨æˆ·éœ€æ±‚
        html: HTML å†…å®¹
        user_goal_requires_interaction: ç”¨æˆ·ç›®æ ‡æ˜¯å¦éœ€è¦äº¤äº’

    Returns:
        å¢å¼ºçš„ Sense Prompt
    """
    return f"""ä½ æ˜¯ä¸€ä¸ªç½‘é¡µç»“æ„åˆ†æä¸“å®¶ã€‚è¯·ç”Ÿæˆ Python ä»£ç åˆ†æä»¥ä¸‹ç½‘é¡µçš„ DOM ç»“æ„ã€‚

ã€ä»»åŠ¡ç›®æ ‡ã€‘
ç«™ç‚¹ URL: {url}
ç”¨æˆ·éœ€æ±‚: {user_goal}

ã€HTML å†…å®¹ï¼ˆå‰ 10000 å­—ç¬¦ï¼‰ã€‘
{html[:10000]}

ã€ä»£ç è¦æ±‚ã€‘
1. ä½¿ç”¨ BeautifulSoup è§£æ HTML
2. ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ã€å¯ç›´æ¥è¿è¡Œçš„ Python è„šæœ¬
3. **æµ‹è¯•å¤šä¸ªé€‰æ‹©å™¨**å¹¶è¿”å›æœ‰æ•ˆçš„
4. **æ£€æµ‹æ˜¯å¦éœ€è¦äº¤äº’**ï¼ˆå¦‚ç‚¹å‡»æœç´¢æŒ‰é’®ï¼‰
5. è¾“å‡º JSON æ ¼å¼çš„åˆ†æç»“æœ

ã€è¾“å‡ºæ ¼å¼ã€‘
```json
{{
  "article_selector": "æ–‡ç« /æ¡ç›®å®¹å™¨çš„ CSS é€‰æ‹©å™¨",
  "title_selector": "æ ‡é¢˜çš„é€‰æ‹©å™¨",
  "link_selector": "é“¾æ¥çš„é€‰æ‹©å™¨",
  "valid_selectors": ["é€‰æ‹©å™¨1", "é€‰æ‹©å™¨2"],  // å®é™…æµ‹è¯•æœ‰æ•ˆçš„é€‰æ‹©å™¨
  "selector_test_results": [
    {{"selector": "a.link", "count": 10, "valid": true}},
    {{"selector": "div.item", "count": 0, "valid": false}}
  ],
  "pagination": {{"type": "next_page|infinite_scroll|load_more|none", "selector": "..."}},
  "requires_interaction": true/false,  // æ˜¯å¦éœ€è¦äº¤äº’ï¼ˆå¦‚ç‚¹å‡»æœç´¢æŒ‰é’®ï¼‰
  "interaction_hints": ["å¯èƒ½éœ€è¦ç‚¹å‡»æœç´¢æŒ‰é’®", "å¯èƒ½éœ€è¦å¡«å†™è¡¨å•"],
  "sample_entries": [
    {{"title": "...", "link": "...", "extra": "..."}}
  ],
  "recommendations": ["å»ºè®®1", "å»ºè®®2"]
}}
```

ã€é€‰æ‹©å™¨æµ‹è¯•è¦æ±‚ã€‘
åœ¨ä»£ç ä¸­æµ‹è¯•ä»¥ä¸‹å¸¸è§é€‰æ‹©å™¨æ¨¡å¼ï¼š
- ç›´æ¥é€‰æ‹©å™¨: `a.card-link`, `article h2 a`
- çˆ¶å­é€‰æ‹©å™¨: `div.card-list a`, `ul.items li a`
- å±æ€§é€‰æ‹©å™¨: `[href*="/p/"]`, `[class*="title"]`
- ç»„åˆé€‰æ‹©å™¨: `article.post a[href]`

ã€äº¤äº’æ£€æµ‹è¦æ±‚ã€‘
æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«ï¼š
- æœç´¢è¡¨å•/æœç´¢æŒ‰é’®
- "åŠ è½½æ›´å¤š"æŒ‰é’®
- åˆ†é¡µé“¾æ¥
- éœ€è¦ç‚¹å‡»æ‰èƒ½å±•å¼€çš„å†…å®¹

ã€ä»£ç æ¨¡æ¿ã€‘
```python
from bs4 import BeautifulSoup
import json
import sys

html = '''{html[:5000]}'''

soup = BeautifulSoup(html, 'lxml')

# æµ‹è¯•é€‰æ‹©å™¨
test_selectors = [
    # æ ¹æ®å®é™…é¡µé¢è°ƒæ•´
    'a[href]',
    'article a',
    '[class*="title"]',
    '[href*="/p/"]',
]

selector_results = []
for selector in test_selectors:
    elements = soup.select(selector)
    selector_results.append({{
        "selector": selector,
        "count": len(elements),
        "valid": len(elements) > 0
    }})

# æ‰¾å‡ºæœ‰æ•ˆçš„é€‰æ‹©å™¨
valid_selectors = [r["selector"] for r in selector_results if r["valid"]]

# æ£€æµ‹æ˜¯å¦éœ€è¦äº¤äº’
requires_interaction = False
interaction_hints = []

if soup.find('input', type='search') or soup.find('button', string=lambda s: s and 'search' in s.lower()):
    requires_interaction = True
    interaction_hints.append("æ£€æµ‹åˆ°æœç´¢æ¡†æˆ–æœç´¢æŒ‰é’®")

if soup.find('a', string=lambda s: s and 'more' in s.lower()):
    requires_interaction = True
    interaction_hints.append("æ£€æµ‹åˆ°'åŠ è½½æ›´å¤š'é“¾æ¥")

analysis = {{
    "article_selector": "è¯·æ ¹æ® HTML åˆ†æ",
    "title_selector": "è¯·æ ¹æ® HTML åˆ†æ",
    "link_selector": "è¯·æ ¹æ® HTML åˆ†æ",
    "valid_selectors": valid_selectors,
    "selector_test_results": selector_results,
    "pagination": {{"type": "none", "selector": ""}},
    "requires_interaction": requires_interaction,
    "interaction_hints": interaction_hints,
    "sample_entries": [],
    "recommendations": []
}}

print(json.dumps(analysis, ensure_ascii=False, indent=2))
```

è¯·åªè¾“å‡º Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""


# ============================================================================
# å…¼å®¹æ€§ï¼šä¿ç•™æ—§åç§°ï¼ˆç”¨äºå‘åå…¼å®¹ï¼‰
# ============================================================================

# æ—§çš„å¸¸é‡åï¼Œç°åœ¨ç”¨å‡½æ•°æ›¿ä»£
SENSE_DOM_ANALYSIS_PROMPT = ""  # ä½¿ç”¨ get_sense_dom_analysis_prompt()
CODE_GENERATION_PROMPT = ""     # ä½¿ç”¨ get_code_generation_prompt()
CODE_DIAGNOSE_PROMPT = ""       # ä½¿ç”¨ get_code_diagnose_prompt()
CODE_REPAIR_PROMPT = ""         # ä½¿ç”¨ get_code_repair_prompt()
QUALITY_EVALUATION_PROMPT = ""  # ä½¿ç”¨ get_quality_evaluation_prompt()
REPORT_GENERATION_PROMPT = ""   # ä½¿ç”¨ get_report_generation_prompt()


# ============================================================================
# å¯¼å‡ºï¼ˆæ–°å¢æ·±åº¦éªŒè¯å‡½æ•°ï¼‰
# ============================================================================

__all__ = [
    # åŸºç¡€å‡½æ•°
    "extract_python_code",
    # Sense èŠ‚ç‚¹
    "get_sense_dom_analysis_prompt",
    "get_enhanced_sense_prompt",  # æ–°å¢ï¼šå¢å¼ºçš„ Sense Promptï¼ˆå¸¦é€‰æ‹©å™¨éªŒè¯ï¼‰
    # Plan èŠ‚ç‚¹
    "get_code_generation_prompt",
    "get_code_generation_prompt_with_memory",  # æ–°å¢ï¼šå¸¦è®°å¿†çš„ä»£ç ç”Ÿæˆ
    # Interact èŠ‚ç‚¹ï¼ˆæ–°å¢ï¼‰
    "get_interact_prompt",
    # SOOAL èŠ‚ç‚¹
    "get_code_diagnose_prompt",
    "get_code_repair_prompt",
    # Verify èŠ‚ç‚¹
    "get_quality_evaluation_prompt",
    "get_enhanced_quality_evaluation_prompt",
    "get_deep_validation_prompt",
    "extract_validation_rules",
    # Reflexion èŠ‚ç‚¹ï¼ˆæ–°å¢ï¼‰
    "get_reflection_prompt",
    # Report èŠ‚ç‚¹
    "get_report_generation_prompt",
    # å¸¸é‡ï¼ˆå‘åå…¼å®¹ï¼‰
    "AVAILABLE_TOOLS",
]


# ============================================================================
# å¯ç”¨å·¥å…·è¯´æ˜ï¼ˆç»™ LLM å‚è€ƒï¼‰
# ============================================================================

AVAILABLE_TOOLS = """
## å¯ç”¨çš„å·¥å…·å’Œåº“

### Browser (Playwright Sync)
```python
from playwright.sync_api import sync_playwright

def scrape(url: str) -> dict:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()  # æ­£ç¡®çš„ API
        page.goto(url)
        page.wait_for_selector('body', timeout=10000)
        content = page.inner_text(selector)
        browser.close()
```

### Parser (BeautifulSoup)
```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(html, 'lxml')
items = soup.select('.item-class')
```

### æ•°æ®è¾“å‡º
```python
import json
output = {{
    "results": data_list,
    "metadata": {{"total": len(data_list)}}
}}
print(json.dumps(output, ensure_ascii=False))
```
"""


# ============================================================================
# Reflexion èŠ‚ç‚¹ Prompts - åæ€å’Œè®°å¿†å¢å¼º
# ============================================================================

def get_reflection_prompt(
    url: str,
    user_goal: str,
    execution_result: dict,
    sample_data: list,
    generated_code: str,
    previous_reflections: list = None,
) -> str:
    """
    ç”Ÿæˆ Reflexion é˜¶æ®µçš„åæ€ Prompt

    åŸºäº Reflexion è®ºæ–‡ (arXiv:2303.11366) çš„ Act-Reflect-Remember å¾ªç¯ï¼Œ
    è®© LLM æ·±åº¦åˆ†æå¤±è´¥åŸå› å¹¶ç”Ÿæˆç»“æ„åŒ–åæ€ã€‚

    Args:
        url: ç›®æ ‡ç«™ç‚¹ URL
        user_goal: ç”¨æˆ·éœ€æ±‚
        execution_result: æ‰§è¡Œç»“æœ
        sample_data: æå–çš„æ•°æ®æ ·æœ¬
        generated_code: ç”Ÿæˆçš„ä»£ç 
        previous_reflections: å†å²åæ€è®°å½•

    Returns:
        åæ€ Prompt
    """
    import json

    # åˆ†ææ‰§è¡Œç»“æœ
    success = execution_result.get("success", False)
    error = execution_result.get("error", "")
    stderr = execution_result.get("stderr", "")
    data_count = len(sample_data)

    # æ„å»ºå†å²åæ€æ–‡æœ¬
    previous_reflections_text = ""
    if previous_reflections:
        previous_reflections_text = "\n## å†å²åæ€ï¼ˆæœ€è¿‘3æ¬¡ï¼‰\n"
        for i, refl in enumerate(previous_reflections[-3:], 1):
            previous_reflections_text += f"{i}. {refl}\n"
    else:
        previous_reflections_text = "\n## å†å²åæ€\nï¼ˆæ— ï¼‰\n"

    # æ ·æœ¬æ•°æ®é¢„è§ˆ
    sample_preview = json.dumps(sample_data[:3], ensure_ascii=False) if sample_data else "[]"

    return f"""ä½ æ˜¯ä¸€ä¸ªWebçˆ¬è™«ä¸“å®¶ï¼Œæ­£åœ¨åˆ†æä¸€æ¬¡å¤±è´¥çš„çˆ¬è™«å°è¯•ã€‚

## ä»»åŠ¡ä¿¡æ¯
- URL: {url}
- ç›®æ ‡: {user_goal}

## æ‰§è¡Œç»“æœ
- æ‰§è¡ŒæˆåŠŸ: {success}
- æå–æ•°æ®é‡: {data_count}æ¡
- é”™è¯¯ä¿¡æ¯: {error[:500] if error else "æ— "}
- æ ‡å‡†é”™è¯¯è¾“å‡º: {stderr[:500] if stderr else "æ— "}

## ç”Ÿæˆçš„ä»£ç ï¼ˆå‰2000å­—ç¬¦ï¼‰
```python
{generated_code[:2000]}
```

## æå–çš„æ•°æ®æ ·æœ¬
```json
{sample_preview}
```

{previous_reflections_text}

## è¯·è¿›è¡Œæ·±åº¦åæ€

åˆ†æè¿™æ¬¡å¤±è´¥çš„åŸå› ï¼Œå¹¶æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

### 1. å¤±è´¥ç±»å‹
ä»ä»¥ä¸‹ç±»å‹ä¸­é€‰æ‹©ä¸€ä¸ªï¼š
- **selector_error**: CSSé€‰æ‹©å™¨ä¸åŒ¹é…å…ƒç´ 
- **js_rendering**: JavaScriptå†…å®¹æœªæ­£ç¡®æ¸²æŸ“
- **timeout**: é¡µé¢åŠ è½½æˆ–æ“ä½œè¶…æ—¶
- **rate_limit**: è¢«é€Ÿç‡é™åˆ¶æˆ–å°ç¦
- **empty_result**: æ‰§è¡ŒæˆåŠŸä½†æ— æ•°æ®æå–
- **syntax_error**: ä»£ç è¯­æ³•é”™è¯¯
- **api_error**: Playwright APIä½¿ç”¨é”™è¯¯
- **blocked**: è¢«åçˆ¬è™«ç³»ç»Ÿé˜»æ­¢
- **other**: å…¶ä»–åŸå› 

### 2. æ ¹æœ¬åŸå› 
å…·ä½“åˆ†æä¸ºä»€ä¹ˆå¤±è´¥ï¼Œä¸è¦æ³›æ³›è€Œè°ˆã€‚

### 3. ä¸‹æ¬¡åº”è¯¥å°è¯•çš„æ–¹æ³•
ç»™å‡ºå…·ä½“çš„ã€å¯æ“ä½œçš„ä¿®å¤å»ºè®®ã€‚

### 4. é¿å…é‡å¤
è¯´æ˜ä¸‹æ¬¡åº”è¯¥é¿å…ä»€ä¹ˆï¼Œç¡®ä¿ä¸é‡å¤ç›¸åŒçš„é”™è¯¯ã€‚

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
```json
{{
    "failure_type": "selector_error",
    "root_cause": "å…·ä½“çš„æ ¹æœ¬åŸå› åˆ†æ...",
    "suggested_fix": "å…·ä½“çš„ä¿®å¤å»ºè®®...",
    "avoid_repeat": "ä¸‹æ¬¡åº”è¯¥é¿å…..."
}}
```

**é‡è¦**: å¦‚æœè¿™æ˜¯ç¬¬2æ¬¡æˆ–æ›´å¤šæ¬¡å°è¯•ï¼Œè¯·ç¡®ä¿ä½ çš„åˆ†æä¸å†å²åæ€ä¸åŒï¼Œæ‰¾åˆ°æ–°çš„è§’åº¦ï¼
"""


def get_code_generation_prompt_with_memory(
    url: str,
    user_goal: str,
    dom_analysis: str,
    failure_history: list = None,
    reflection_memory: list = None,
    successful_patterns: list = None,
    iteration: int = 0,
) -> str:
    """
    ç”Ÿæˆå¸¦å†å²è®°å¿†çš„ä»£ç ç”Ÿæˆ Prompt

    åœ¨åŸæœ‰ä»£ç ç”Ÿæˆ Prompt åŸºç¡€ä¸Šï¼ŒåŠ å…¥ï¼š
    1. å¤±è´¥å†å² - é¿å…é‡å¤é”™è¯¯
    2. åæ€æ€»ç»“ - åˆ©ç”¨ç»éªŒæ”¹è¿›
    3. æˆåŠŸæ¨¡å¼ - å‚è€ƒæœ‰æ•ˆæ–¹æ³•
    4. è¿­ä»£æ¬¡æ•° - æ˜ç¡®å½“å‰è¿›åº¦

    Args:
        url: ç›®æ ‡ç«™ç‚¹ URL
        user_goal: ç”¨æˆ·éœ€æ±‚
        dom_analysis: DOM åˆ†æç»“æœ
        failure_history: å¤±è´¥å†å²è®°å½•
        reflection_memory: åæ€è®°å¿†
        successful_patterns: æˆåŠŸæ¨¡å¼
        iteration: å½“å‰è¿­ä»£æ¬¡æ•°

    Returns:
        ä»£ç ç”Ÿæˆ Prompt
    """
    import json

    # æ£€æµ‹æ˜¯å¦éœ€è¦ä»£ç ç‰‡æ®µæå–
    needs_code_extraction = _detect_code_snippet_need(user_goal)

    code_extraction_guide = ""
    if needs_code_extraction:
        code_extraction_guide = """

ã€ä»£ç ç‰‡æ®µæå–ï¼ˆSVG/HTMLï¼‰ã€‘
å¦‚æœç”¨æˆ·éœ€æ±‚åŒ…å«"SVGä»£ç "ã€"HTMLä»£ç ç‰‡æ®µ"ã€"å¯Œæ–‡æœ¬"ã€"å›¾æ ‡"ç­‰å…³é”®è¯ï¼š
- ä½¿ç”¨ `page.inner_html()` æˆ– `element.inner_html()` æå– HTML/SVG ä»£ç 
- ä½¿ç”¨ `page.evaluate("el => el.outerHTML")` è·å–åŒ…å«å…ƒç´ è‡ªèº«çš„å®Œæ•´ä»£ç 
- ç­‰å¾… JS åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆ: `page.wait_for_selector('svg', timeout=15000)`

æå–ç¤ºä¾‹ï¼š
```python
# æå– SVG ä»£ç 
svgs = page.locator("svg").all()
for svg in svgs[:5]:  # é™é‡é‡‡æ ·
    svg_code = svg.evaluate("el => el.outerHTML")
    results.append({{"svg_code": svg_code, "type": "svg"}})

# æå– HTML ç‰‡æ®µ
html_blocks = page.locator(".rich-text, .description, [data-html]").all()
for block in html_blocks[:5]:
    html_snippet = block.inner_html()
    results.append({{"html_snippet": html_snippet, "type": "html"}})
```
"""

    # æ„å»ºå†å²ç»éªŒéƒ¨åˆ†
    memory_section = ""

    if failure_history:
        memory_section += "\n## âš ï¸ å¤±è´¥å†å²ï¼ˆè¯·é¿å…é‡å¤ï¼‰\n"
        for i, fail in enumerate(failure_history[-3:], 1):
            memory_section += f"""
### å°è¯• #{i}
- å¤±è´¥ç±»å‹: {fail.get('failure_type', 'unknown')}
- åŸå› : {fail.get('root_cause', 'unknown')[:200]}
- æ•°æ®é‡: {fail.get('data_count', 0)}æ¡
- å»ºè®®: {fail.get('suggested_fix', 'æ— ')[:200]}
"""

    if reflection_memory:
        memory_section += "\n## ğŸ“ åæ€æ€»ç»“\n"
        for i, refl in enumerate(reflection_memory[-3:], 1):
            memory_section += f"{i}. {refl[:300]}\n"

    if successful_patterns:
        memory_section += f"\n## âœ… æˆåŠŸæ¨¡å¼ï¼ˆå¯ä»¥å‚è€ƒï¼‰\n"
        for pattern in successful_patterns:
            memory_section += f"- {pattern}\n"

    if iteration > 0:
        memory_section += f"\n---\n\n**âš ï¸ è¿™æ˜¯ç¬¬ {iteration + 1} æ¬¡å°è¯•ã€‚è¯·ç¡®ä¿ä¸é‡å¤ä¹‹å‰çš„é”™è¯¯ï¼**\n"

    return f"""ä½ æ˜¯ä¸€ä¸ªçˆ¬è™«ä»£ç ç”Ÿæˆä¸“å®¶ã€‚è¯·ç”Ÿæˆå®Œæ•´çš„çˆ¬è™«ä»£ç ã€‚

ã€ä»»åŠ¡ç›®æ ‡ã€‘
ç«™ç‚¹ URL: {url}
ç”¨æˆ·éœ€æ±‚: {user_goal}

{memory_section}

ã€DOM åˆ†æç»“æœã€‘
{dom_analysis}
{code_extraction_guide}

ã€ä»£ç è¦æ±‚ã€‘
1. ä½¿ç”¨ **playwright.sync_api**ï¼ˆåŒæ­¥æ¨¡å¼ï¼Œä¸æ˜¯ asyncï¼ï¼‰
2. æ­£ç¡®çš„ API è°ƒç”¨ï¼š
   - `browser = p.chromium.launch(headless=True)`
   - `page = browser.new_page()`  â† æ­£ç¡®ï¼
   - ä¸è¦ä½¿ç”¨ `browser.new_context()` â† é”™è¯¯ï¼
3. æå–çš„æ•°æ®ä»¥ JSON æ ¼å¼è¾“å‡ºåˆ° stdout
4. **ç¡®ä¿ä¸é‡å¤ä¹‹å‰çš„é”™è¯¯**ï¼šå¦‚æœå¤±è´¥å†å²æåˆ°é€‰æ‹©å™¨é—®é¢˜ï¼Œè¯·ä½¿ç”¨ä¸åŒçš„é€‰æ‹©å™¨ç­–ç•¥

ã€å¸¸è§é”™è¯¯é¿å…ã€‘
| é”™è¯¯å†™æ³• | æ­£ç¡®å†™æ³• |
|---------|---------|
| `browser.new_context()` | `browser.new_page()` |
| `await page.goto()` | `page.goto()` (åŒæ­¥æ¨¡å¼) |
| `async def scrape()` | `def scrape()` (åŒæ­¥å‡½æ•°) |
| å¿˜è®° `import json` | å¿…é¡»åœ¨é¡¶éƒ¨å¯¼å…¥ |
| ç¡¬ç¼–ç å•ä¸€é€‰æ‹©å™¨ | å‡†å¤‡å¤‡é€‰é€‰æ‹©å™¨ |

ã€è¾“å‡ºæ ¼å¼ã€‘
```json
{{
  "results": [{{"field1": "value1", ...}}],
  "metadata": {{"total_pages": 1, "sample_size": N}}
}}
```

ã€ä»£ç æ¨¡æ¿ã€‘
```python
from playwright.sync_api import sync_playwright
import json

def scrape(url: str) -> dict:
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()  # æ­£ç¡®çš„ API

        page.goto(url, wait_until='domcontentloaded', timeout=30000)

        # ç­‰å¾…å†…å®¹åŠ è½½ - ä½¿ç”¨å¤šç§ç­–ç•¥ç¡®ä¿æˆåŠŸ
        try:
            page.wait_for_selector('body', timeout=10000)
        except:
            pass

        results = []

        # TODO: æ ¹æ® DOM åˆ†æç»“æœå®ç°æ•°æ®æå–
        # å‚è€ƒ: {dom_analysis[:500]}

        # å¦‚æœç¬¬ä¸€æ¬¡å°è¯•æ²¡è·å–åˆ°æ•°æ®ï¼Œå°è¯•å¤‡é€‰æ–¹æ³•
        if not results:
            # TODO: å®ç°å¤‡é€‰æå–ç­–ç•¥
            pass

        browser.close()

        return {{
            "results": results,
            "metadata": {{"total_pages": 1, "sample_size": len(results)}}
        }}

if __name__ == "__main__":
    import sys
    url = sys.argv[1] if len(sys.argv) > 1 else "{url}"
    result = scrape(url)
    print(json.dumps(result, ensure_ascii=False, indent=2))
```

è¯·åªè¾“å‡ºå®Œæ•´å¯æ‰§è¡Œçš„ Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""


# ============================================================================
# Phase 1: Validation Node Prompts
# ============================================================================

def get_validation_prompt(
    url: str,
    user_goal: str,
    failed_selectors: list,
    html: str,
) -> str:
    """
    ç”Ÿæˆé€‰æ‹©å™¨éªŒè¯ Prompt

    å½“åˆå§‹é€‰æ‹©å™¨éªŒè¯å¤±è´¥æ—¶ï¼Œç”Ÿæˆæ›¿ä»£é€‰æ‹©å™¨å»ºè®®ã€‚
    """
    failed_text = "\n".join(f"- {s}" for s in failed_selectors) if failed_selectors else "æ— "

    return f"""ä½ æ˜¯ä¸€ä¸ªç½‘é¡µç»“æ„åˆ†æä¸“å®¶ã€‚è¯·ç”Ÿæˆ Python ä»£ç å¯»æ‰¾æ›¿ä»£çš„ CSS é€‰æ‹©å™¨ã€‚

ã€ä»»åŠ¡ç›®æ ‡ã€‘
ç«™ç‚¹ URL: {url}
ç”¨æˆ·éœ€æ±‚: {user_goal}

ã€éªŒè¯å¤±è´¥çš„é€‰æ‹©å™¨ã€‘
{failed_text}

ã€HTML å†…å®¹ï¼ˆå‰ 10000 å­—ç¬¦ï¼‰ã€‘
{html[:10000]}

ã€ä»£ç è¦æ±‚ã€‘
ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ Python è„šæœ¬ï¼Œç”¨äºï¼š
1. åˆ†æ HTML ç»“æ„
2. ç”Ÿæˆå¤‡é€‰é€‰æ‹©å™¨
3. æµ‹è¯•è¿™äº›é€‰æ‹©å™¨
4. è¿”å›æœ‰æ•ˆçš„é€‰æ‹©å™¨åˆ—è¡¨

ã€è¾“å‡ºæ ¼å¼ã€‘
```json
{{
  "alternative_selectors": ["é€‰æ‹©å™¨1", "é€‰æ‹©å™¨2", "é€‰æ‹©å™¨3"],
  "test_results": [
    {{"selector": "...", "count": 10, "valid": true}},
    {{"selector": "...", "count": 0, "valid": false}}
  ],
  "recommendations": ["å»ºè®®1", "å»ºè®®2"]
}}
```

è¯·åªè¾“å‡º Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""


def get_verify_plan_prompt(
    url: str,
    user_goal: str,
    code: str,
    validation_report: dict,
) -> str:
    """
    ç”Ÿæˆä»£ç è®¡åˆ’éªŒè¯ Prompt

    åœ¨å®é™…æ‰§è¡Œå‰éªŒè¯ä»£ç çš„æ­£ç¡®æ€§ã€‚
    """
    import json
    validation_text = json.dumps(validation_report, ensure_ascii=False) if validation_report else "{}"

    return f"""è¯·éªŒè¯ä»¥ä¸‹çˆ¬è™«ä»£ç çš„æ­£ç¡®æ€§ã€‚

ã€ä»»åŠ¡ç›®æ ‡ã€‘
ç«™ç‚¹ URL: {url}
ç”¨æˆ·éœ€æ±‚: {user_goal}

ã€éªŒè¯æŠ¥å‘Šã€‘
{validation_text}

ã€ç”Ÿæˆçš„ä»£ç ã€‘
```python
{code[:5000]}
```

ã€è¯·æ£€æŸ¥ã€‘
1. è¯­æ³•é”™è¯¯
2. ç¼ºå¤±çš„å¯¼å…¥
3. API ä½¿ç”¨æ­£ç¡®æ€§
4. é”™è¯¯å¤„ç†
5. èµ„æºé‡Šæ”¾ï¼ˆbrowser.close()ï¼‰

è¯·è¾“å‡º JSON æ ¼å¼çš„éªŒè¯ç»“æœã€‚
"""


# ============================================================================
# Phase 2: Stealth-First Default Prompts
# ============================================================================

def get_stealth_code_generation_prompt(
    url: str,
    user_goal: str,
    dom_analysis: str,
    stealth_level: str = "medium",
) -> str:
    """
    ç”Ÿæˆå¸¦éšèº«é…ç½®çš„ä»£ç ç”Ÿæˆ Prompt

    åœ¨ä»£ç ç”Ÿæˆæ—¶è‡ªåŠ¨åŒ…å«éšèº«æµè§ˆå™¨é…ç½®ã€‚
    """
    # æ£€æµ‹æ˜¯å¦éœ€è¦ä»£ç ç‰‡æ®µæå–
    needs_code_extraction = _detect_code_snippet_need(user_goal)

    code_extraction_guide = ""
    if needs_code_extraction:
        code_extraction_guide = """

ã€ä»£ç ç‰‡æ®µæå–ï¼ˆSVG/HTMLï¼‰ã€‘
å¦‚æœç”¨æˆ·éœ€æ±‚åŒ…å«"SVGä»£ç "ã€"HTMLä»£ç ç‰‡æ®µ"ã€"å¯Œæ–‡æœ¬"ã€"å›¾æ ‡"ç­‰å…³é”®è¯ï¼š
- ä½¿ç”¨ `page.inner_html()` æˆ– `element.inner_html()` æå– HTML/SVG ä»£ç 
- ä½¿ç”¨ `page.evaluate("el => el.outerHTML")` è·å–åŒ…å«å…ƒç´ è‡ªèº«çš„å®Œæ•´ä»£ç 
- ç­‰å¾… JS åŠ¨æ€å†…å®¹åŠ è½½å®Œæˆ: `page.wait_for_selector('svg', timeout=15000)`
"""

    # æ ¹æ®éšèº«ç­‰çº§è·å–é…ç½®
    stealth_configs = {
        "none": {
            "launch_args": "[]",
            "delay": "0",
            "stealth_script": "# æ— éšèº«è„šæœ¬",
        },
        "low": {
            "launch_args": '["--disable-blink-features=AutomationControlled"]',
            "delay": "random.uniform(1, 2)",
            "stealth_script": """
        # åŸºç¡€éšèº«è„šæœ¬
        page.add_init_script('''
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
        ''')""",
        },
        "medium": {
            "launch_args": '["--disable-blink-features=AutomationControlled", "--no-sandbox"]',
            "delay": "random.uniform(2, 4)",
            "stealth_script": """
        # éšèº«è„šæœ¬
        page.add_init_script('''
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            window.chrome = {runtime: {}, loadTimes: function() {}};
            Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]};
        ''')""",
        },
        "high": {
            "launch_args": '["--disable-blink-features=AutomationControlled", "--no-sandbox", "--disable-web-security"]',
            "delay": "random.uniform(3, 6)",
            "stealth_script": """
        # é«˜çº§éšèº«è„šæœ¬
        page.add_init_script('''
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            window.chrome = {runtime: {}, loadTimes: function() {}, csi: function() {}};
            Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]};
            Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']};
            const originalQuery = window.navigator.permissions.query;
            window.navigator.permissions.query = (parameters) => (
                parameters.name === 'notifications' ?
                    Promise.resolve({{state: Notification.permission}}) :
                    originalQuery(parameters)
            );
        ''')""",
        },
    }

    config = stealth_configs.get(stealth_level, stealth_configs["medium"])

    return f"""ä½ æ˜¯ä¸€ä¸ªçˆ¬è™«ä»£ç ç”Ÿæˆä¸“å®¶ã€‚è¯·ç”Ÿæˆå®Œæ•´çš„çˆ¬è™«ä»£ç ã€‚

ã€ä»»åŠ¡ç›®æ ‡ã€‘
ç«™ç‚¹ URL: {url}
ç”¨æˆ·éœ€æ±‚: {user_goal}

ã€âš ï¸ éšèº«é…ç½®ã€‘
éšèº«ç­‰çº§: {stealth_level}
æ­¤ç½‘ç«™æ£€æµ‹åˆ°åçˆ¬è™«æªæ–½ï¼Œå¿…é¡»ä½¿ç”¨éšèº«æµè§ˆå™¨é…ç½®ï¼

ã€DOM åˆ†æç»“æœã€‘
{dom_analysis}
{code_extraction_guide}

ã€ä»£ç è¦æ±‚ã€‘
1. ä½¿ç”¨ **playwright.sync_api**ï¼ˆåŒæ­¥æ¨¡å¼ï¼‰
2. **å¿…é¡»ä½¿ç”¨ä»¥ä¸‹éšèº«é…ç½®**ï¼š
   - launch_args: [{config["launch_args"]}]
   - éšæœºå»¶è¿Ÿ: {config["delay"]} ç§’
{config["stealth_script"]}
3. æ­£ç¡®çš„ API è°ƒç”¨ï¼š`page = browser.new_page()`
4. æå–çš„æ•°æ®ä»¥ JSON æ ¼å¼è¾“å‡ºåˆ° stdout

ã€å¸¸è§é”™è¯¯é¿å…ã€‘
| é”™è¯¯å†™æ³• | æ­£ç¡®å†™æ³• |
|---------|---------|
| `browser.new_context()` | `browser.new_page()` |
| `await page.goto()` | `page.goto()` (åŒæ­¥æ¨¡å¼) |
| å¿˜è®° `browser.close()` | å¿…é¡»å…³é—­æµè§ˆå™¨é‡Šæ”¾èµ„æº |

ã€è¾“å‡ºæ ¼å¼ã€‘
```json
{{
  "results": [{{"field1": "value1", ...}}],
  "metadata": {{"total_pages": 1, "sample_size": N}}
}}
```

è¯·åªè¾“å‡ºå®Œæ•´å¯æ‰§è¡Œçš„ Python ä»£ç ï¼Œä¸è¦æœ‰å…¶ä»–è¯´æ˜ã€‚
"""


# ============================================================================
# Phase 4: Deep Reflection Prompts
# ============================================================================

def get_deep_reflection_prompt(
    url: str,
    user_goal: str,
    execution_result: dict,
    sample_data: list,
    generated_code: str,
    previous_reflections: list = None,
    website_type: str = "unknown",
    anti_bot_level: str = "none",
    website_features: list = None,
    partial_success: dict = None,
) -> str:
    """
    ç”Ÿæˆæ·±åº¦åæ€ Prompt

    Phase 4 å¢å¼ºï¼šåŒ…å«ç½‘ç«™ç±»å‹ã€åçˆ¬è™«ç­‰çº§ã€ç‰¹å¾ã€éƒ¨åˆ†æˆåŠŸæ•°æ®
    """
    import json

    # åˆ†ææ‰§è¡Œç»“æœ
    success = execution_result.get("success", False)
    error = execution_result.get("error", "")
    stderr = execution_result.get("stderr", "")
    data_count = len(sample_data)

    # æ„å»ºå†å²åæ€æ–‡æœ¬
    previous_reflections_text = ""
    if previous_reflections:
        previous_reflections_text = "\n## å†å²åæ€ï¼ˆæœ€è¿‘3æ¬¡ï¼‰\n"
        for i, refl in enumerate(previous_reflections[-3:], 1):
            previous_reflections_text += f"{i}. {refl}\n"
    else:
        previous_reflections_text = "\n## å†å²åæ€\nï¼ˆæ— ï¼‰\n"

    # æ ·æœ¬æ•°æ®é¢„è§ˆ
    sample_preview = json.dumps(sample_data[:3], ensure_ascii=False) if sample_data else "[]"

    # ç½‘ç«™ç‰¹å¾æ–‡æœ¬
    features_text = ", ".join(website_features) if website_features else "æ— "

    # éƒ¨åˆ†æˆåŠŸæ•°æ®æ–‡æœ¬
    partial_text = ""
    if partial_success:
        partial_text = f"""
## éƒ¨åˆ†æˆåŠŸåˆ†æ
- æ˜¯å¦éƒ¨åˆ†æˆåŠŸ: {partial_success.get('partial_success', False)}
- æˆåŠŸç‡: {partial_success.get('success_rate', 0):.1%}
- ä¼˜åŠ¿: {', '.join(partial_success.get('strengths', []))}
- é—®é¢˜: {', '.join(partial_success.get('issues', []))}
"""

    return f"""ä½ æ˜¯ä¸€ä¸ªWebçˆ¬è™«ä¸“å®¶ï¼Œæ­£åœ¨è¿›è¡Œæ·±åº¦åæ€åˆ†æã€‚

## ä»»åŠ¡ä¿¡æ¯
- URL: {url}
- ç›®æ ‡: {user_goal}

## ç½‘ç«™åˆ†æï¼ˆPhase 4 å¢å¼ºï¼‰
- ç½‘ç«™ç±»å‹: {website_type}
- åçˆ¬è™«ç­‰çº§: {anti_bot_level}
- æ£€æµ‹åˆ°çš„ç‰¹å¾: {features_text}
{partial_text}

## æ‰§è¡Œç»“æœ
- æ‰§è¡ŒæˆåŠŸ: {success}
- æå–æ•°æ®é‡: {data_count}æ¡
- é”™è¯¯ä¿¡æ¯: {error[:500] if error else "æ— "}
- æ ‡å‡†é”™è¯¯è¾“å‡º: {stderr[:500] if stderr else "æ— "}

## ç”Ÿæˆçš„ä»£ç ï¼ˆå‰2000å­—ç¬¦ï¼‰
```python
{generated_code[:2000]}
```

## æå–çš„æ•°æ®æ ·æœ¬
```json
{sample_preview}
```

{previous_reflections_text}

## è¯·è¿›è¡Œæ·±åº¦åæ€

åŸºäºç½‘ç«™ç±»å‹ï¼ˆ{website_type}ï¼‰å’Œåçˆ¬è™«ç­‰çº§ï¼ˆ{anti_bot_level}ï¼‰ï¼Œåˆ†æè¿™æ¬¡å¤±è´¥çš„åŸå› ã€‚

### 1. å¤±è´¥ç±»å‹
ä»ä»¥ä¸‹ç±»å‹ä¸­é€‰æ‹©ä¸€ä¸ªï¼š
- **selector_error**: CSSé€‰æ‹©å™¨ä¸åŒ¹é…å…ƒç´ 
- **js_rendering**: JavaScriptå†…å®¹æœªæ­£ç¡®æ¸²æŸ“
- **timeout**: é¡µé¢åŠ è½½æˆ–æ“ä½œè¶…æ—¶
- **rate_limit**: è¢«é€Ÿç‡é™åˆ¶æˆ–å°ç¦
- **empty_result**: æ‰§è¡ŒæˆåŠŸä½†æ— æ•°æ®æå–
- **syntax_error**: ä»£ç è¯­æ³•é”™è¯¯
- **api_error**: Playwright APIä½¿ç”¨é”™è¯¯
- **blocked**: è¢«åçˆ¬è™«ç³»ç»Ÿé˜»æ­¢
- **anti_bot**: åçˆ¬è™«ç³»ç»Ÿï¼ˆCAPTCHAã€Cloudflareç­‰ï¼‰
- **other**: å…¶ä»–åŸå› 

### 2. æ ¹æœ¬åŸå› 
ç»“åˆç½‘ç«™ç±»å‹å’Œåçˆ¬è™«ç­‰çº§ï¼Œåˆ†æå…·ä½“çš„æ ¹æœ¬åŸå› ã€‚

### 3. ä¸‹æ¬¡åº”è¯¥å°è¯•çš„æ–¹æ³•
ç»™å‡ºå…·ä½“çš„ã€å¯æ“ä½œçš„ä¿®å¤å»ºè®®ï¼Œè€ƒè™‘ï¼š
- å¯¹äº {website_type} ç±»å‹çš„ç½‘ç«™
- å¯¹äº {anti_bot_level} çº§åˆ«çš„åçˆ¬è™«
- åŸºäºéƒ¨åˆ†æˆåŠŸæ•°æ®ä¸­çš„ä¼˜åŠ¿/é—®é¢˜

### 4. é¿å…é‡å¤
è¯´æ˜ä¸‹æ¬¡åº”è¯¥é¿å…ä»€ä¹ˆï¼Œç¡®ä¿ä¸é‡å¤ç›¸åŒçš„é”™è¯¯ã€‚

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºï¼š
```json
{{
    "failure_type": "selector_error",
    "root_cause": "å…·ä½“çš„æ ¹æœ¬åŸå› åˆ†æ...",
    "suggested_fix": "å…·ä½“çš„ä¿®å¤å»ºè®®...",
    "avoid_repeat": "ä¸‹æ¬¡åº”è¯¥é¿å…..."
}}
```

**é‡è¦**: å¦‚æœè¿™æ˜¯ç¬¬2æ¬¡æˆ–æ›´å¤šæ¬¡å°è¯•ï¼Œè¯·ç¡®ä¿ä½ çš„åˆ†æä¸å†å²åæ€ä¸åŒï¼Œæ‰¾åˆ°æ–°çš„è§’åº¦ï¼
"""


# ============================================================================
# Update __all__ exports
# ============================================================================

__all__ = [
    # åŸºç¡€å‡½æ•°
    "extract_python_code",
    # Sense èŠ‚ç‚¹
    "get_sense_dom_analysis_prompt",
    "get_enhanced_sense_prompt",
    # Plan èŠ‚ç‚¹
    "get_code_generation_prompt",
    "get_code_generation_prompt_with_memory",
    # Interact èŠ‚ç‚¹
    "get_interact_prompt",
    # SOOAL èŠ‚ç‚¹
    "get_code_diagnose_prompt",
    "get_code_repair_prompt",
    # Verify èŠ‚ç‚¹
    "get_quality_evaluation_prompt",
    "get_enhanced_quality_evaluation_prompt",
    "get_deep_validation_prompt",
    "extract_validation_rules",
    # Reflexion èŠ‚ç‚¹
    "get_reflection_prompt",
    "get_deep_reflection_prompt",  # Phase 4
    # Report èŠ‚ç‚¹
    "get_report_generation_prompt",
    # Phase 1: Validation
    "get_validation_prompt",
    "get_verify_plan_prompt",
    # Phase 2: Stealth
    "get_stealth_code_generation_prompt",
    # å¸¸é‡ï¼ˆå‘åå…¼å®¹ï¼‰
    "AVAILABLE_TOOLS",
]
